# PCR Benchmark Results

Point-in-time benchmark results for PCR v0.1.0.
Generated by `scripts/shell/run_benchmarks.sh` — see [BENCHMARKING.md](BENCHMARKING.md) to reproduce.

The full standalone HTML report (with embedded charts and glyph gallery images) is at
`benchmark_results/latest/benchmark_report.html`.

---

## System

| | |
|---|---|
| **CPU** | Intel Core i5-9400F @ 2.90 GHz |
| **Cores** | 6 |
| **RAM** | 7.7 GB |
| **GPU** | Not available (CPU-only run) |
| **PCR** | v0.1.0 |
| **Date** | 2026-02-23 |

---

## CPU vs GPU Throughput

*CPU baseline values; GPU numbers from a separate GPU-enabled run (see README.md).*

| Points | Mode | Time (s) | Mpts/s | Speedup |
|-------:|:-----|--------:|------:|:--------|
| 1M | CPU | 0.251 | 3.99 | baseline |
| 1M | CPU-MT | 0.243 | 4.12 | 1.0× |
| 1M | GPU-Host | 0.081 | 12.32 | **3.1×** |
| 1M | GPU-Device | 0.067 | 14.95 | **3.7×** |
| 5M | CPU | 2.369 | 2.11 | baseline |
| 5M | CPU-MT | 2.291 | 2.18 | 1.0× |
| 5M | GPU-Host | 0.150 | 33.39 | **15.8×** |
| 5M | GPU-Device | 0.093 | 54.02 | **25.6×** |
| 10M | CPU | 6.045 | 1.65 | baseline |
| 10M | GPU-Host | 0.405 | 24.72 | **14.9×** |
| 10M | GPU-Device | 0.156 | 64.06 | **38.7×** |
| 25M | CPU | 17.883 | 1.40 | baseline |
| 25M | GPU-Device | 0.224 | 111.70 | **79.9×** |

---

## CPU Multi-Thread Scaling

*i5-9400F, 6 cores. CPU throughput is memory-bandwidth-bound — adding threads past 2 doesn't help on this chip.*

| Points | Mode | Time (s) | Mpts/s | Speedup |
|-------:|:-----|--------:|------:|:--------|
| 1M | CPU-1T | 0.247 | 4.05 | baseline |
| 1M | CPU-2T | 0.234 | 4.28 | 1.1× |
| 1M | CPU-4T | 0.257 | 3.89 | 1.0× |
| 1M | CPU-NT | 0.243 | 4.11 | 1.0× |
| 5M | CPU-1T | 2.219 | 2.25 | baseline |
| 5M | CPU-NT | 2.466 | 2.03 | 0.9× |
| 10M | CPU-1T | 5.805 | 1.72 | baseline |
| 10M | CPU-NT | 6.535 | 1.53 | 0.9× |
| 25M | CPU-1T | 18.132 | 1.38 | baseline |
| 25M | CPU-NT | 19.475 | 1.28 | 0.9× |

> The i5-9400F's memory bandwidth is saturated at ~1T. Adding threads causes cache contention. On multi-socket or high-bandwidth machines, scaling is positive.

---

## Hybrid Mode

| Points | Mode | Time (s) | Mpts/s | Speedup |
|-------:|:-----|--------:|------:|:--------|
| 1M | CPU-MT | 0.383 | 2.61 | baseline |
| 1M | GPU-Device | 0.076 | 13.11 | **5.0×** |
| 1M | Hybrid-NT | 0.207 | 4.82 | 1.9× |
| 5M | CPU-MT | 2.518 | 1.99 | baseline |
| 5M | GPU-Device | 0.089 | 56.36 | **28.4×** |
| 5M | Hybrid-NT | 2.326 | 2.15 | 1.1× |
| 10M | CPU-MT | 6.095 | 1.64 | baseline |
| 10M | GPU-Device | 0.148 | 67.46 | **41.1×** |
| 10M | Hybrid-NT | 5.823 | 1.72 | 1.1× |
| 25M | CPU-MT | 19.198 | 1.30 | baseline |
| 25M | GPU-Device | 0.231 | 108.33 | **83.2×** |
| 25M | Hybrid-NT | 18.028 | 1.39 | 1.1× |

---

## Glyph Rendering Throughput

*1000×1000 grid, CPU-only, best-of-3. Cells marked `—` exceeded the 30 s timeout.*

| Glyph | N=100K (s) | Mpts/s | N=1M (s) | Mpts/s | N=5M (s) | Mpts/s |
|:------|----------:|------:|---------:|------:|---------:|------:|
| Point | 0.037 | 2.67 | 0.335 | 2.98 | 2.782 | 1.80 |
| Line hl=1 | 0.032 | 3.12 | 0.296 | 3.37 | 2.712 | 1.84 |
| Line hl=4 | 0.035 | 2.83 | 0.305 | 3.27 | 2.974 | 1.68 |
| Line hl=16 | 0.044 | 2.29 | 0.438 | 2.28 | 3.730 | 1.34 |
| Gauss σ=1 | 0.079 | 1.27 | 0.746 | 1.34 | 5.443 | 0.92 |
| Gauss σ=4 | 0.643 | 0.16 | 6.387 | 0.16 | — | — |
| Gauss σ=16 | 8.915 | 0.01 | — | — | — | — |

**Key takeaway:** Gaussian cost scales with σ². On CPU, σ=4 is ~40× slower than Point for the same N; σ=16 is ~900×. Use GPU for large σ.

---

## DC LiDAR — Real-World Benchmark

*Washington DC open LiDAR dataset. 10 tiles (30.3 M points), 1 m cell size, elevation.*

| Mode | Wall (s) | I/O (s) | Library (s) | Mpts/s | Speedup |
|:-----|--------:|-------:|------------:|------:|:--------|
| CPU-1T | 16.2 | 10.4 | 5.4 | 5.61 | baseline |
| CPU-MT | 11.1 | 5.9 | 5.0 | 6.05 | 1.0× |
| GPU | 8.3 | 6.4 | 1.8 | 17.28 | **2.9×** |
| Hybrid | 11.6 | 6.3 | 5.3 | 5.74 | 0.9× |

I/O is 54–77% of total wall time across all modes. GPU reduces library time from 5.0 s → 1.8 s (2.9×), but the overall speedup is limited by disk reads.

For the full 188-tile (479 M point) run, see [DC_LIDAR_BENCHMARK_COMPREHENSIVE.md](DC_LIDAR_BENCHMARK_COMPREHENSIVE.md).

---

## Glyph Visual Gallery

The gallery below was generated by `scripts/patterns/generate_glyph_patterns.py`. Images live in
`glyph_pattern_outputs/` and are embedded in the HTML report at `benchmark_results/latest/benchmark_report.html`.

| Image | Description |
|-------|-------------|
| `01_gap_fill_comparison.png` | Point (blocky, 8% coverage) vs Gaussian σ=2 vs σ=6 on sparse data |
| `02_sigma_progression.png` | Gaussian σ from 0.5 to 16 — smoothing effect on a sinusoidal surface |
| `03_anisotropic_gaussian.png` | Isotropic, X-elongated, Y-elongated, rotated 30°/45°/75° kernels |
| `04_line_directions.png` | 9-direction sweep + half-length 2 → 32 progression |
| `05_flow_field.png` | Vortex flow field — Line glyph vs Adaptive Gaussian |
| `06_sparse_vs_dense.png` | N=50 / 500 / 5,000 points — Point vs Gaussian side-by-side |
| `07_per_point_sigma.png` | Per-point σ ∝ distance from cloud centre (adaptive footprints) |
| `08_glyph_showcase.png` | Concentric bullseye rings — Point / Line / Gauss σ=2 / Gauss σ=5 |

---

## Reproducing These Results

```bash
# 1. Build
cmake --build /workspace/build --target _pcr -j$(nproc)

# 2. Run full suite (CPU-only, ~10–15 min)
scripts/shell/run_benchmarks.sh

# 3. Open the report
open benchmark_results/latest/benchmark_report.html
```

For GPU results, pass `--no-gpu` to force CPU-only, or run without the flag on a CUDA-capable machine.
